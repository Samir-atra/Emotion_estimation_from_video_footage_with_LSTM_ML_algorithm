- to create the second dataset that contain the happy sad and neutral emotions.
- loaded the fer2013 dataset, take only the happy, sad and neutral emotions 3, 4 and 6.
- split into training, validation and test sets.
- run it through mediapipe to get only valid and understood images.
- find the relevant blendshapes for happy, neutral and sad emotions, sum to 33 blendshapes.
- augment the training set, takeout the ununderstandable images and add the augmented ones. end up with around 30000 images augmented and non-augmented

- create a blendshapes dataset.
- - before this point I was using mean squared error loss function, but after an objection from/discussion with Muhamed Riza in class about the loss function, 
I had to look again and found that categorical cross entropy is the more suitable one. 

- chenged the happy, sad, and neutral labels to 0,1,2, and transfered them to categorical (one-hot) instead of numbers.
- train the model, and compile with categorical crossentropy and adam optimizer with metrics, accuracy and f1-score 
- implement keras tuner for the learning rate and check for improvement.

experiments record:

- training with learning rate = 0.00005 for 100 epochs nothing normalized gives: 
    loss: 0.6518 - accuracy: 0.7014 - f1_score: 0.6602 - val_loss: 0.6610 - val_accuracy: 0.6920 - val_f1_score: 0.6572
- training with learning rate = 0.00005 for 100 epochs train and val normalized gives:
    loss: 0.6358 - accuracy: 0.7094 - f1_score: 0.6693 - val_loss: 0.6490 - val_accuracy: 0.6956 - val_f1_score: 0.6555
    in keras tuner 0.001 is best so far when accuracy is not included.
    when include accuracy and train on the above learning rate, the gradient explodes after around 40 epochs.
    use weight initialization and gradient clipping to tackle that.
- using the gradient clipping, a learning rate = 0.0046 and after the 100 epoch:
    loss: 0.5600 - categorical_accuracy: 0.7457 - f1_score: 0.7129 - val_loss: 0.6512 - val_categorical_accuracy: 0.6966 - val_f1_score: 0.6490
    in this case after running for more than 100 epochs, will give a few jumps in the loss that means the training is failing but 
    accuracy and f1-score stay steadily improving.
- try to improve the fit and reduce bias by adding another lstm layer, keeping the LR and epochs same:
    loss: 0.5220 - categorical_accuracy: 0.7618 - f1_score: 0.7307 - val_loss: 0.6777 - val_categorical_accuracy: 0.6935 - val_f1_score: 0.6604
    after evaluating and testing this one, it does not work because the result will be normalized. so will retrain with out normalization.
- removed normalization and training with same LR and architecture as above to get the following results:
    loss: 0.5789 - categorical_accuracy: 0.7375 - f1_score: 0.7030 - val_loss: 0.6428 - val_categorical_accuracy: 0.7141 - val_f1_score: 0.6738 
    when implemt the model to the gaze project it gives solid results.
    the full model setup is: 
    -   model = tf.keras.Sequential([
        tf.keras.layers.LSTM(units = 33, activation = 'relu', return_sequences= True),
        tf.keras.layers.LSTM(units = 12, activation = 'relu', return_sequences= False),
        tf.keras.layers.Dense(units = 3, activation = 'softmax'),
        ])


        # Compiling the RNN
        model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
                    optimizer= tf.keras.optimizers.Adam(learning_rate = 0.00464489028556945, clipnorm=1),
                    metrics = [tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.F1Score()])


        # Fitting the RNN to the Training set
        model.fit(x=X_train, y=Y_train, validation_data = (X_val,y_val) ,epochs = 100)



=================================================================================================================================================

to further improve the model: 
try regularization for overfitting then test with different optimizers,  
get the confusion matrix 
get spontanious dataset 


running keras tuner: 
best so far: 
Hyperparameters: 
lr: 0.00020047766795977846 
unites: 8 
beta: 0.7 
Score: 0.7140657305717468 

and: 
Best val_categorical_accuracy So Far: 0.7125256657600403 
|Best Value So Far |Hyperparameter 
|0.0010147         |lr 
|18                |unites 
|0.9               |beta 


- after a few trials without improvement, checked the counts of the classes of the dataset, and equalized them to 7500 each after around 13000, 7500, and 9000, and added a dataset shuffling before training. 
after fixing the dataset it shows significant improvements but it has overfitting so will use dropout and kernel initialization with uniform_glorot to avoid that. 

- after increasing the number of neurons for the hidden layers and maybe they look too much in comparison to the number of the features but experiments give good results and the number of hidden layers themselves, found that with tanh as activation function the model improve the training result but overfit while with relu, it does not get to 70% 

- after a few experiments with different model architectures from making the lstm bidirectional to adding kernel regularizer and recurrent dropout only variance improved but the bias stayed the same, turned to the optimizer again, and noticed the ema_momentum has a big effect on the accuracy so continued and included amsgrad, and run for 300 epochs  

 - after so many trials with lstm and fully dense model think that the quality of the data is affecting the results and better the 73% accuracy is not possible. 

so found the confusion matrix for the dense model to be: 

[[746, 11, 76], 
[ 53, 279, 163], 
[ 51, 293, 242]] 

and for the LSTM test 73% from past week is: 

[[754, 22, 57], 
[ 58, 209, 228], 
[ 54,  91, 441]] 

both cases shows that the major issue happens between the sad and neutral images, and from previous experience the neutral face would be predicted as sad rather than happy 
when the dataset does not include neutral class, so it is either more work on the model or the final product is not so reasonable. 
so lstm is better, and that is encouraging for further experimentation, so will keep the idea of changing the dataset aside for now and work on lstm. 
- now keeping the learning rate in the range e-4 to e-5 and increasing the size of the model to increase its capacity since it stops improving after some epochs with big bias. 
By increasing the number of neurons and hidden layers and using activation function selu because the relu makes the loss explode after a few epochs probably 
because does not have a negative part while tanh works fast but stops improving early, reasoned because the slope goes to zero for big values so from the photo 

 
 and that showed improvement but took much time to get results since the epoch takes more time and more epohs are required. 

 
There will not be a test model this week. 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

- after training for 600 epochs reached 71 validation accuracy again with a steady and smooth improvement line for loss, accuracy and f1 score. 
so now will train for 1000 epochs and then decide what's next.

- started creating the full dataset that includes the happy, sad and unknown emotions and that is done by the following:
    - take 4000 images from the happy and sad classes since this is the maximum number possible while keeping the classes equal.
    - from the other classes take 1500 each and since the disgust class has only around 500 so it will be reasonably equal to the other 
    classes in the dataset.
    - for the validation and test sets included all images with the full count for each of them.

- truncate the floating points of the dataset for 4 digits only then found that the sad class will disappear if this is the case.

- still training on the same model. while changing the Hyperparameters such as regularization lambda, number of hidden layer, number of units, 
learning rate and others, and some ideas are assigning weights to the classes or initializing the kernels with a certain range.
- also visualizing the features might give an idea about what is happening, there are 5 non-relevant features.
[[235 42 14] 

[120 786 204]  

[ 23 117 105]] 

And the test metrics for the model shared are: 

categorical_accuracy: 0.6788 - categorical_crossentropy: 0.7182 - f1_score: 0.6169 - loss: 0.7185 

- the main issue now is that the unknown class is taking over the happy and sad classes. 


------------------------------------------------------------------------------------------------------------------------------------------
- for the model  epoch:886-val_loss:0.7097.keras  the metrics are: 
loss: 0.6918 - categorical_crossentropy: 0.6914 - categorical_accuracy: 0.7005 - f1_score: 0.6209

and the confusion matrix is:
[[243  35  13]
 [121 813 176]
 [ 23 125  97]]

 when changing the batch size from 128 to 256 it shows some behavior of delay, since it takes more time to reach the same results.
- using the 150 batch size since it is the square root of the number of training instances. and it shows a consistant improvement in the first
20 epochs, so will work using it.

things to do:
- normalize then split the dataset and apply the mean and standard deviation on all sets.
- save the mean and sdv to normalize the camera footage before feeding into the model.
- create scatter plots for features instead of normal plots.
- create scatter plots for class vs. features.
- calculate correlation factors and create correlation plots.

after training for 5000 epochs, the two best checkpoint test metrics are:
the first epoch:3636-val_loss:0.6552.keras is:
loss: 0.6283 - categorical_crossentropy: 0.6280 - categorical_accuracy: 0.7139 - f1_score: 0.6354

[[241  41   9]
 [ 96 836 178]
 [ 18 129  98]]

and the second epoch:4437-val_loss:0.6506.keras is:
loss: 0.6238 - categorical_crossentropy: 0.6235 - categorical_accuracy: 0.7199 - f1_score: 0.6298

[[251  35   5]
 [110 850 150]
 [ 21 140  84]]

 - for these models applying on the gaze project it shows a good behavior but a neutral face is sad with out eye contact otherwise it is unknown.
 that is one obvious flow.
 - with smaller network and less numbers of epochs the happy emotion gets less understandable and the sadness understanding increase significantly.
 - running an even smaller model will get the happy and sad almose the same understandablity but confuse them with unknown.

 - when normalizing found thea it will give a reversed effect since the blendshapes are already probabilities.
 - enlarge the images and interpolate, is one idea to improve the model performance quality.

 - the first model 'epoch_55-val_loss_0.6160.keras' trained with weight decay and tanh as activation function for all the hidden layers shows an improvement with metrics:
 loss: 0.5949 - categorical_crossentropy: 0.5949 - categorical_accuracy: 0.7570 - f1_score: 0.6129

 [[216  71   4]
 [ 62 982  66]
 [ 14 184  47]]

 - sent a follow up email about multipie dataset
 - found the correlation matrix of the features from the different classes.

- find the images in the test set that are being mis classified, after indexing the dataset and reverse access them.
- each class good and bad, and the values of the features that are well classified and the range that is not well classified.

_______________________________________________________________________________________________________________________________________________________________________________


- look into the probabilities of the different classes
- try fine tuning the model
- check whisper for nlp
- the problem owner is happy with LSTM so far and he will be waiting for the final report

- to find the images that get mispredicted, recreated the testset, by adding an extra column with an index number and the same numbers get 
assigned to the blendshapes of the images in the blends dataset, then predict the classes using the model and creat a list with the indices of the 
mispredicted images, to visualize them later and find if there is a pattern between them.

- doing this shows that the misclassified images also contains a type of noise such as a hand covering the face partially or a cup of drink or big facial hair 
so it is obvious and does not require further investigation

- sent an email to Marijn asking about the dropout unexpected behavior
- checked for the non relevant blendshapes in the augmented training set and found that 34 only are the relevant ones for a dataset including all the emotions
so recreated the blendshapes datasets and will train on them.
- starting by running keras tuner to find best parameters