{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samir-atra/BDSD_Minor_Project/blob/main/MediaPipe_Stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQbjSccqvaoD",
        "outputId": "c7d1fc21-bd3e-4fc9-9d98-0a23e0ff21ab"
      },
      "outputs": [],
      "source": [
        "# Download and install packages for colab\n",
        "# !python -m pip install mediapipe\n",
        "# !wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
        "# !wget -q -O image.png https://storage.googleapis.com/mediapipe-assets/business-person.png\n",
        "\n",
        "# Imports\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pathlib\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import csv\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_tuner\n",
        "import math\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzbR9n38yXoA"
      },
      "outputs": [],
      "source": [
        "# MediaPipe visualizations\n",
        "\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  face_landmarks_list = detection_result.face_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected faces to visualize.\n",
        "  for idx in range(len(face_landmarks_list)):\n",
        "    face_landmarks = face_landmarks_list[idx]\n",
        "\n",
        "    # Draw the face landmarks.\n",
        "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    face_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
        "    ])\n",
        "\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
        "        landmark_drawing_spec=None,\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "        .get_default_face_mesh_tesselation_style())\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
        "        landmark_drawing_spec=None,\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "        .get_default_face_mesh_contours_style())\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
        "          landmark_drawing_spec=None,\n",
        "          connection_drawing_spec=mp.solutions.drawing_styles\n",
        "          .get_default_face_mesh_iris_connections_style())\n",
        "\n",
        "  return annotated_image\n",
        "\n",
        "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
        "  # Extract the face blendshapes category names and scores.\n",
        "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
        "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
        "  # The blendshapes are ordered in decreasing score value.\n",
        "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(12, 12))\n",
        "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
        "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
        "  ax.invert_yaxis()\n",
        "\n",
        "  # Label each bar with values\n",
        "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
        "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
        "\n",
        "  ax.set_xlabel('Score')\n",
        "  ax.set_title(\"Face Blendshapes\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a function to write dta into csv files\n",
        "def csv_writer(filename, fields, data):\n",
        "\n",
        "    csvfile = filename\n",
        "    fields = fields\n",
        "    with open(csvfile, mode=\"a\") as first:\n",
        "        csvwriter = csv.writer(first)\n",
        "        csvwriter.writerow(fields)\n",
        "        csvwriter.writerows(data)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wehz7zYlzMjZ"
      },
      "outputs": [],
      "source": [
        "# download and upload images from and into colab\n",
        "\n",
        "# download\n",
        "img = cv2.imread(\"images.jpg\")\n",
        "cv2_imshow(img)\n",
        "\n",
        "# upload images to colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded:\n",
        "  content = uploaded[filename]\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(content)\n",
        "\n",
        "if len(uploaded.keys()):\n",
        "  IMAGE_FILE = next(iter(uploaded))\n",
        "  print('Uploaded file:', IMAGE_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QKppKp8rzg3u",
        "outputId": "de368a9b-3d41-40ad-8126-9a06378c03b0"
      },
      "outputs": [],
      "source": [
        "#inferencing and visualizing\n",
        "\n",
        "# STEP 2: Create an FaceLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
        "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
        "                                       output_face_blendshapes=True,\n",
        "                                       output_facial_transformation_matrixes=True,\n",
        "                                       num_faces=1)\n",
        "detector = vision.FaceLandmarker.create_from_options(options)\n",
        "\n",
        "# STEP 3: Load the input image.\n",
        "image = mp.Image.create_from_file(\"image.png\")\n",
        "\n",
        "# STEP 4: Detect face landmarks from the input image.\n",
        "detection_result = detector.detect(image)\n",
        "# print(detection_result.facial_transformation_matrixes)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "plt.imshow(annotated_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "AeWx7TJ2nIfz",
        "outputId": "9edb1bfb-caf7-47ff-ce07-a22b969ba819"
      },
      "outputs": [],
      "source": [
        "# plot bar graphs for specific image blendshapes\n",
        "\n",
        "plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])\n",
        "# print(detection_result.face_blendshapes[0])\n",
        "print(detection_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset analysis/calculations\n",
        "\n",
        "categories_counts = {\"0\":0, \"1\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0, \"6\":0}\n",
        "skipped = {\"0\":0, \"1\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0, \"6\":0}\n",
        "\n",
        "\n",
        "with open(\"fer2013.csv\", mode= \"r\") as data:                    # open the dataset file\n",
        "  csvFile = csv.reader(data)\n",
        "  next(csvFile)\n",
        "  for lines in csvFile:                                         # loop throught the training instances\n",
        "    if lines[0] in categories_c:\n",
        "      categories_counts[lines[0]] = categories_counts[lines[0]] + 1\n",
        "    image = np.array(str(lines[1]).split(' ')).reshape(48, 48, 1).astype(np.uint8)     # build the image from a list of numbers\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB,data=image)\n",
        "    detection_result = detector.detect(rgb_frame)                                       # detect the face in the image\n",
        "    # check if the image get recognized or skipped by mediapipe\n",
        "    if detection_result.face_blendshapes == []:                                         # check if mediapipe is able to detect a face in the image\n",
        "        skipped[lines[0]] = skdfivjipped[lines[0]] + 1\n",
        "    else:\n",
        "        # img = plt.imshow(image)\n",
        "        # plt.show()\n",
        "        continue\n",
        "print(categories_counts)\n",
        "print(skipped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5W9V1SX2QKO"
      },
      "outputs": [],
      "source": [
        "# # Processing, cleaning and visualizing the fer2023 dataset  \n",
        "\n",
        "fullset = []\n",
        "Training_set = []\n",
        "validation_set = []\n",
        "test_set = []\n",
        "class_counter = {\"0\":0, \"1\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0, \"6\":0}\n",
        "\n",
        "\n",
        "with open(\"training_set_full.csv\", mode= \"r\") as data:\n",
        "  csvFile = csv.reader(data)\n",
        "  next(csvFile)\n",
        "  # iterate over the dataset and append the images to a list, for limited numbers according to it's classes, to creat the training set\n",
        "  for lines in csvFile:\n",
        "      if lines[0] == \"0\" and class_counter['0'] < 1500:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '1'\n",
        "        fullset.append(lines)\n",
        "      elif lines[0] == \"1\" and class_counter['1'] < 1500:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '1'\n",
        "        fullset.append(lines)\n",
        "      elif lines[0] == '2' and class_counter['2'] < 1500:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '1'\n",
        "        fullset.append(lines) \n",
        "      elif lines[0] == '3' and class_counter['3'] < 4000:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '0'\n",
        "        fullset.append(lines) \n",
        "      elif lines[0] == '4' and class_counter['4'] < 4000:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '2'\n",
        "        fullset.append(lines) \n",
        "      elif lines[0] == '5' and class_counter['5'] < 1500:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '1'\n",
        "        fullset.append(lines) \n",
        "      elif lines[0] == '6' and class_counter['6'] < 1500:\n",
        "        class_counter[lines[0]] = class_counter[lines[0]] + 1\n",
        "        lines[0] = '1'\n",
        "        fullset.append(lines) \n",
        "\n",
        "print(class_counter)\n",
        "# plot the required image with annotations\n",
        "plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])\n",
        "annotated_image = draw_landmarks_on_image(rgb_frame.numpy_view(), detection_result)\n",
        "cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cleaning the dataset \n",
        "\n",
        "training_set = []\n",
        "validation_set = []\n",
        "test_set = []\n",
        "\n",
        "# creat lists for dataset splits\n",
        "for i in fullset:\n",
        "    if i[2] == 'Training':\n",
        "        training_set.append(i)\n",
        "    elif i[2] == 'PublicTest':\n",
        "        validation_set.append(i)\n",
        "    elif i[2] == 'PrivateTest':\n",
        "        test_set.append(i)\n",
        "\n",
        "training_set_hus = []\n",
        "validation_set_hus = []\n",
        "test_set_hus = []\n",
        "\n",
        "# append the understandable images of the original dataset splits to new lists\n",
        "for lines in training_set:\n",
        "      image = np.array(lines[1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    #   plt.imshow(image)\n",
        "    #   plt.show()\n",
        "      frame = mp.Image(image_format=mp.ImageFormat.SRGB,data=image)\n",
        "      detection_result = detector.detect(frame)\n",
        "      if detection_result.face_blendshapes == []:\n",
        "        continue\n",
        "      else:\n",
        "        training_set_hus.append(lines)\n",
        "\n",
        "for val_line in validation_set:\n",
        "      imagee = np.array(val_line[1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "      imagee = cv2.cvtColor(imagee, cv2.COLOR_GRAY2RGB)\n",
        "    #   plt.imshow(image)\n",
        "    #   plt.show()\n",
        "      framee = mp.Image(image_format=mp.ImageFormat.SRGB,data=imagee)\n",
        "      detection_result = detector.detect(framee)\n",
        "      if detection_result.face_blendshapes == []:\n",
        "        continue\n",
        "      else:\n",
        "        validation_set_hus.append(val_line)\n",
        "\n",
        "for test_lines in test_set:\n",
        "      imageee = np.array(test_lines[1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "      imageee = cv2.cvtColor(imageee, cv2.COLOR_GRAY2RGB)\n",
        "    #   plt.imshow(image)\n",
        "    #   plt.show()\n",
        "      frameee = mp.Image(image_format=mp.ImageFormat.SRGB,data=imageee)\n",
        "      detection_result = detector.detect(frameee)\n",
        "      if detection_result.face_blendshapes == []:\n",
        "        continue\n",
        "      else:\n",
        "        test_set_hus.append(test_lines)\n",
        "\n",
        "# create files for the new dataset splits for the created lists\n",
        "fields = [\"emotion\", \"pixels\", \"Usage\"]\n",
        "csv_writer(\"training_set_full.csv\", fields, training_set_hus)\n",
        "csv_writer(\"validation_set_full.csv\", fields, validation_set_hus)\n",
        "csv_writer(\"test_set_full.csv\", fields, test_set_hus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# choosing relevant blendshapes for the happy, sad dataset and the happy, sad, neutral dataset \n",
        "\n",
        "\n",
        "# create a dictionary with numbers from 0 to 52 \n",
        "blend_shapes = dict()\n",
        "for i in range(0,52):\n",
        "    blend_shapes[str(i)] = 0\n",
        "\n",
        "print(blend_shapes)\n",
        "# sad = 0\n",
        "# happy = 0\n",
        "counter = 0\n",
        "\n",
        "  # find which blendshapes are most relevant to happiness and sadness by passing them on a 0.4 threshold \n",
        "for i in range(len(training_set_hus)):\n",
        "    image = np.array(training_set_hus[i][1].split(' ')).reshape(48, 48, 1).astype(np.uint8)    # build images from pixels lists\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    frame = mp.Image(image_format=mp.ImageFormat.SRGB,data=image)\n",
        "    detection_result = detector.detect(frame)\n",
        "    if detection_result.face_blendshapes == []:\n",
        "        continue\n",
        "    else:\n",
        "        counter += 1              \n",
        "    if counter %500 == 0:\n",
        "        time.sleep(5)                 # sleep 5 seconds to avoid processor overload\n",
        "    for i in detection_result.face_blendshapes[0]:           # compare each blendshape in for the current image with the threshold\n",
        "        if i.score > 0.4:\n",
        "            blend_shapes[str(i.index)] = blend_shapes[str(i.index)] + 1           # edit the counts dictionary\n",
        "print(blend_shapes)\n",
        "\n",
        "# from the resulting dictionary, manually find the modst relevent blendshapes and insert them in a list\n",
        "\n",
        "blends_to_print = ['1', '2', '3', '4', '5', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '25', '34', '35', '38', '44', '45', '46', '47', '48', '49', 'emotion'] # list of blendshapes indices that are most relevent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# indexing the testset\n",
        "\n",
        "test_set_hus = []\n",
        "\n",
        "with open(\"test_set_full.csv\", mode= \"r\") as data:\n",
        "  csvFile = csv.reader(data)\n",
        "  next(csvFile)\n",
        "  for lines in csvFile:\n",
        "      imageee = np.array(lines[1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "      imageee = cv2.cvtColor(imageee, cv2.COLOR_GRAY2RGB)\n",
        "    #   plt.imshow(image)\n",
        "    #   plt.show()\n",
        "      frameee = mp.Image(image_format=mp.ImageFormat.SRGB,data=imageee)\n",
        "      detection_result = detector.detect(frameee)\n",
        "      if detection_result.face_blendshapes == []:\n",
        "        continue\n",
        "      else:\n",
        "        test_set_hus.append(lines)\n",
        "        \n",
        "nums = np.array([i for i in range(0,1648)])               # create a list of numbers to be appended as indices\n",
        "nums = np.reshape(nums, (1648,1))\n",
        "test_set_hus = np.array(test_set_hus)\n",
        "test_set_hus= np.delete(test_set_hus,2,1)                 # delete the third column which contains the name of the split\n",
        "test_set_hus = np.hstack((test_set_hus, nums))            # add the indices column\n",
        "fields = [\"emotion\", \"pixels\", \"Index\"]\n",
        "csv_writer(\"test_set_full_index.csv\", fields, test_set_hus)          # creat an indexed dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "deiWIq0NLwog",
        "outputId": "8e8fc28d-4c13-4a78-93b2-68d03a1c56b8"
      },
      "outputs": [],
      "source": [
        "# Augmenting the training set images\n",
        "\n",
        "\n",
        "augmented_training_set = []\n",
        "training_images = []\n",
        "training_labels = []\n",
        "\n",
        "# create an image list and a labels list for the training dataset\n",
        "for i in range(math.floor(len(training_set_hus))):\n",
        "    image = np.array(training_set_hus[i][1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    training_images.append(image)\n",
        "    training_labels.append(int(training_set_hus[i][0]))\n",
        "\n",
        "\n",
        "# rescaling and augmenting images models\n",
        "rescaling1 = tf.keras.Sequential([ \n",
        "  tf.keras.layers.Rescaling(1./255)                         # scale down the images pixel values\n",
        "])\n",
        "\n",
        "rescaling2 = tf.keras.Sequential([                          # scale up the pixel values\n",
        "  tf.keras.layers.Rescaling(1.*255)\n",
        "])\n",
        "\n",
        "augment = tf.keras.Sequential([                             # augment by random flipping and random rotation\n",
        "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "  tf.keras.layers.RandomRotation(0.1)\n",
        "])\n",
        "\n",
        "for ele in range(len(training_images)):\n",
        "# scale down the image and augment\n",
        "  img = training_images[ele]\n",
        "  label = training_labels[ele]\n",
        "  image = rescaling1(img)\n",
        "  aug_image = augment(image)\n",
        "#   scale up the image cast to an integer and transform into a numpy array for it to be understood by mediapipe\n",
        "  aug_image = rescaling2(aug_image)\n",
        "  aug_image = tf.cast(aug_image, tf.uint8)\n",
        "  aug_image = np.array(aug_image)\n",
        "  flatten_image = aug_image.flatten()                                            # flatten the augmented image, to be used in creating the csv file\n",
        "  flat_aug_image = [flatten_image[i] for i in range(0,len(flatten_image),3)]\n",
        "  # flattt = np.reshape(flat_aug_image,(48,48))\n",
        "  # plt.imshow(flattt)\n",
        "  # plt.show()\n",
        "  frame = mp.Image(image_format=mp.ImageFormat.SRGB,data=aug_image)\n",
        "  detection_result = detector.detect(frame)\n",
        "  if detection_result.face_blendshapes == []:\n",
        "    continue\n",
        "  else:\n",
        "    element = [training_labels[ele]]\n",
        "    for i in flat_aug_image:\n",
        "      element.append(i)\n",
        "    augmented_training_set.append([element[0],str(element[1:]).replace(',',\"\").replace('[','').replace(']',''),'Training'])\n",
        "\n",
        "for images in augmented_training_set:\n",
        "  image = np.array(images[1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "csv_writer(\"training_set_full.csv\", ['emotion','pixels'], augmented_training_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQHG7kvVRQI6"
      },
      "outputs": [],
      "source": [
        "# if-else classification ðŸ˜Š\n",
        "\n",
        "# if detection_result.face_blendshapes[0][44].score > 0.5 and detection_result.face_blendshapes[0][45].score > 0.5 :\n",
        "#   print(\"Happy\")\n",
        "# elif detection_result.face_blendshapes[0][42].score > 0.5:\n",
        "#   print(\"Sad\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating blendshapes dataset        (styled)\n",
        "\n",
        "set = []\n",
        "images = []\n",
        "labels = []\n",
        "full_set = []\n",
        "indices = []\n",
        "\n",
        "with open(\"validation_set_full.csv\", mode= \"r\") as data:            # load the dataset that will be processed\n",
        "  csvFile = csv.reader(data)\n",
        "  next(csvFile)\n",
        "  for lines in csvFile:\n",
        "    set.append(lines)\n",
        "  for i in range(len(set)):    \n",
        "    image = np.array(set[i][1].split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    images.append(image)                               # create a list of the images\n",
        "    labels.append(int(set[i][0]))                      # create a list of the labels\n",
        "    # indices.append(int(set[i][2]))\n",
        "    \n",
        "    \n",
        "arr = np.zeros((len(images), 36))                    # create an array with the size of dataset and number of blendshapes to be used \n",
        "\n",
        "# arr[0,:]= blendS_to_print\n",
        "for ele in range(len(images)-1):\n",
        "  img = images[ele]\n",
        "  label = labels[ele]\n",
        "  # img_index = indices[ele]\n",
        "  frame = mp.Image(image_format=mp.ImageFormat.SRGB,data=img)\n",
        "  detection_result = detector.detect(frame)\n",
        "  cat_counter = 0\n",
        "  for category in detection_result.face_blendshapes[0]:\n",
        "    if str(category.index) in blends_to_print:                # if the blendshape from the detector is in the blends_to_pront list then include it in the array\n",
        "      arr[ele, cat_counter] = category.score\n",
        "      cat_counter += 1\n",
        "    else:\n",
        "       continue\n",
        "  arr[ele, 34] = label\n",
        "  # arr[ele, 35] = img_index\n",
        "\n",
        "     \n",
        "# fields = [\"emotion\", \"pixels\", \"Index\"]\n",
        "csv_writer(\"blends_val_all_emotion.csv\",'beedoo', arr)             # create a file from the array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data for LSTM model         ()\n",
        "\n",
        "traindata = []\n",
        "blend_set = []\n",
        "labels_set = []\n",
        "class1=[]\n",
        "class2=[]\n",
        "class3=[]\n",
        "with open(\"blends_train_full_set.csv\", mode= \"r\") as data:\n",
        "  csvFile = csv.reader(data)\n",
        "  next(csvFile)\n",
        "  for line in csvFile:\n",
        "    traindata.append(line[:])\n",
        "  np.random.shuffle(traindata)\n",
        "  for lines in traindata:\n",
        "      blend_set.append(lines[0:52])\n",
        "      labels_set.append(lines[52])\n",
        "blends_set = np.array(blend_set, dtype=np.float64)        # create an array of the blendshapes\n",
        "labels_set = np.array(labels_set, dtype=np.float64)       # create an array of the labels\n",
        "\n",
        "#Reshaping Array\n",
        "X_train = np.reshape(blends_set, (22515, 52,1))\n",
        "Y_train = np.reshape(labels_set, (22515,1)).astype('int')\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=3)       # encode the labels as one-hot\n",
        "\n",
        "# create classes lists for it to be produced as datasets\n",
        "for ins in traindata:\n",
        "  if float(ins[52]) == 0:\n",
        "    class1.append(ins)\n",
        "  elif float(ins[52]) == 1:\n",
        "    class2.append(ins)\n",
        "  elif float(ins[52]) == 2:\n",
        "    class3.append(ins)\n",
        "    \n",
        "csv_writer(\"class1.csv\",'beedoo', class1)\n",
        "csv_writer(\"class2.csv\",'beedoo', class2)\n",
        "csv_writer(\"class3.csv\",'beedoo', class3)\n",
        "\n",
        "# visualizations for the features from the classes and \n",
        "plt.figure(figsize = [40,40])\n",
        "y = np.linspace(0,1999,1999)\n",
        "for j in range(1,52):\n",
        "  plt.subplot(10,6,j)                                         # create a subplot with the number of features\n",
        "  class1_slice = [float(i[j]) for i in blends_set[1:2000]]\n",
        "# class2_slice = [float(i[25]) for i in class2[1:200]]\n",
        "  plt.title(f\"{j}\")\n",
        "  plt.scatter(y,class1_slice,color='blue')\n",
        "plt.scatter(y,class2_slice, color='red')\n",
        "print(class1[1:100])\n",
        "corr_mat = np.corrcoef(class1_slice, class2_slice)            # find the correlation matrix\n",
        "print(corr_mat)\n",
        "\n",
        "# load validation data\n",
        "valdata = []\n",
        "val_blend_set = []\n",
        "val_labels_set = []\n",
        "with open(\"blends_val_all_emotion.csv\", mode= \"r\") as val_data:\n",
        "  csvFile = csv.reader(val_data)\n",
        "  next(csvFile)\n",
        "  for line in csvFile:\n",
        "    valdata.append(line[:])\n",
        "  np.random.shuffle(valdata)\n",
        "  for lines in valdata:\n",
        "      val_blend_set.append(lines[0:34])\n",
        "      val_labels_set.append(lines[34])\n",
        "val_blend_set = np.array(val_blend_set, dtype=np.float64)\n",
        "val_labels_set = np.array(val_labels_set, dtype=np.float64)\n",
        "X_val = np.reshape(val_blend_set, (1657, 34,1))\n",
        "y_val = np.reshape(val_labels_set, (1657,1)).astype('int')\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data normalization\n",
        "\n",
        "mean = tf.math.reduce_mean(X_train,axis=0)            # find the mean for the dataset\n",
        "stddev = tf.math.reduce_std(X_train, axis=0)          # find the standard deviation\n",
        "mean = np.array(mean).T\n",
        "stddev = np.array(stddev).T\n",
        "csv_writer(\"mean_and_std.csv\",'beedoo', mean)\n",
        "csv_writer(\"mean_and_std.csv\",'beedoo', stddev)\n",
        "\n",
        "# normalize data\n",
        "norm = tf.keras.layers.Normalization(axis=1)\n",
        "norm.adapt(X_train)\n",
        "print(X_train[0])\n",
        "XX_train = norm(X_train)\n",
        "print(XX_train[0])\n",
        "XX_train = np.array(X_train)\n",
        "XX_train = X_train[1]\n",
        "plt.scatter(X_train[1],XX_train)\n",
        "\n",
        "norm.adapt(X_train)\n",
        "X_val = norm(X_val)\n",
        "X_val = np.array(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keras tuner for LSTM model\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  \n",
        "  learning_rate = hp.Float(\"lr\", min_value=1e-6, max_value=1e-3, sampling=\"log\")       # learning rate search range\n",
        "  layer_u = hp.Int(\"lu\", min_value=10, max_value=32, step=4)                           # layer units search range\n",
        "  kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")           # kernel regularization search range\n",
        "  acti_f = hp.Choice(\"af\", ['selu', 'tanh', 'relu', 'leaky_relu'])                     # activation function search range\n",
        "  weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")         # weight decay search range\n",
        "  \n",
        "# model structure\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.LSTM(units = 34, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=0.00000195)),\n",
        "      tf.keras.layers.LSTM(units = 26, activation = acti_f, return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r)),\n",
        "      tf.keras.layers.LSTM(units = layer_u, activation = acti_f, return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r)),\n",
        "      tf.keras.layers.LSTM(units = layer_u, activation = acti_f, return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r)),\n",
        "      tf.keras.layers.LSTM(units = layer_u, activation = acti_f, return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r)),\n",
        "      tf.keras.layers.LSTM(units = layer_u, activation = acti_f, return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r)),\n",
        "      tf.keras.layers.LSTM(units = 30, activation = acti_f, return_sequences= False, kernel_regularizer=tf.keras.regularizers.L2(l2=0.00000195)),\n",
        "      tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
        "  ])\n",
        "\n",
        "# another model structure was used for experimenting\n",
        "  # model = tf.keras.Sequential([\n",
        "  #     tf.keras.layers.LSTM(units = 52, activation = acti_f, return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r)),\n",
        "  #     tf.keras.layers.LSTM(units = layer_2, activation = acti_f, return_sequences= True, \n",
        "  #             kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r),kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.1, maxval=0.1)),\n",
        "  #     tf.keras.layers.LSTM(units = layer_3, activation = acti_f, return_sequences= False, \n",
        "  #             kernel_regularizer=tf.keras.regularizers.L2(l2=kernel_r),kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.1, maxval=0.1)),\n",
        "  #     tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
        "  # ])\n",
        "\n",
        "\n",
        "# the dense model structure experimented with\n",
        "  # model = tf.keras.Sequential([ \n",
        "  #     tf.keras.layers.Dense(units = 330, activation = 'relu', batch_input_shape = (64,33)),    \n",
        "  #     tf.keras.layers.Dense(units = 300, activation = 'relu', kernel_regularizer='l2'),\n",
        "  #     tf.keras.layers.Dropout(0.2),                         \n",
        "  #     tf.keras.layers.Dense(units = 250, activation = 'relu', kernel_regularizer='l2'),                         \n",
        "  #     tf.keras.layers.Dropout(0.2),                               \n",
        "  #     tf.keras.layers.Dense(units = 220, activation = 'relu', kernel_regularizer='l2'),   \n",
        "  #     tf.keras.layers.Dropout(0.2),                                                \n",
        "  #     tf.keras.layers.Dense(units = 200, activation = 'relu', kernel_regularizer='l2'), \n",
        "  #     tf.keras.layers.Dropout(0.2),                                                       \n",
        "  #     tf.keras.layers.Dense(units = 150, activation = 'relu', kernel_regularizer='l2'), \n",
        "  #     tf.keras.layers.Dropout(0.2),                                                       \n",
        "  #     tf.keras.layers.Dense(units = 10, activation = 'relu', kernel_regularizer='l2'),                         \n",
        "  #     tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
        "  # ])\n",
        "\n",
        "\n",
        "\n",
        "# Compiling the model\n",
        "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer= tf.keras.optimizers.Adam(learning_rate = learning_rate, global_clipnorm=1, amsgrad = True, weight_decay=weight_d),\n",
        "              metrics = [tf.keras.metrics.CategoricalCrossentropy(), tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.F1Score()])\n",
        "\n",
        "  return model \n",
        "\n",
        "build_model(keras_tuner.HyperParameters())\n",
        "\n",
        "tuner = keras_tuner.RandomSearch(                               # tuner configurations\n",
        "    hypermodel=build_model,\n",
        "    max_trials=15,\n",
        "    objective=keras_tuner.Objective('val_loss', 'min'),\n",
        "    executions_per_trial=1,\n",
        "    overwrite=True,\n",
        "    directory=\"/home/samer/Desktop/Big data Small Data/BDSD/Minor_project/emotion_estimation/\",\n",
        "    project_name=\"Emotion_estimation_tuning\",\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "tuner.search(x=X_train, y=Y_train, validation_data = (X_val,y_val), epochs=100, batch_size = 150)\n",
        "\n",
        "tuner.results_summary()\n",
        "\n",
        "\n",
        "\n",
        "EStop = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=5,restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM model fitting\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.LSTM(units = 33, activation = 'relu', return_sequences= True),\n",
        "#     tf.keras.layers.LSTM(units = 12, activation = 'relu', return_sequences= False),\n",
        "#     tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
        "# ])\n",
        "\n",
        "# the dense model trianed to compare the performance with the LSTM model\n",
        "# model = tf.keras.Sequential([ \n",
        "#     tf.keras.layers.Dense(units = 330, activation = 'relu', batch_input_shape = (64,33)),    \n",
        "#     tf.keras.layers.Dense(units = 300, activation = 'relu', kernel_regularizer='l2'),\n",
        "#     tf.keras.layers.Dropout(0.2),                         \n",
        "#     tf.keras.layers.Dense(units = 250, activation = 'relu', kernel_regularizer='l2'),                         \n",
        "#     tf.keras.layers.Dropout(0.2),                               \n",
        "#     tf.keras.layers.Dense(units = 220, activation = 'relu', kernel_regularizer='l2'),   \n",
        "#     tf.keras.layers.Dropout(0.2),                                                \n",
        "#     tf.keras.layers.Dense(units = 200, activation = 'relu', kernel_regularizer='l2'), \n",
        "#     tf.keras.layers.Dropout(0.2),                                                       \n",
        "#     tf.keras.layers.Dense(units = 150, activation = 'relu', kernel_regularizer='l2'), \n",
        "#     tf.keras.layers.Dropout(0.2),                                                       \n",
        "#     tf.keras.layers.Dense(units = 10, activation = 'relu', kernel_regularizer='l2'),                         \n",
        "#     tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
        "# ])\n",
        "\n",
        "\n",
        "# LSTM model final structure experimented with\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(units = 34, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.LSTM(units = 26, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.LSTM(units = 30, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.LSTM(units = 30, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.LSTM(units = 30, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.LSTM(units = 30, activation = 'selu', return_sequences= True, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.LSTM(units = 30, activation = 'selu', return_sequences= False, kernel_regularizer=tf.keras.regularizers.L2(l2=2.256393134751847e-06)),\n",
        "    tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
        "])\n",
        "\n",
        "# # Compiling \n",
        "# model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#             optimizer= tf.keras.optimizers.Adam(learning_rate = 1.0955e-06, clipnorm=1),\n",
        "#             metrics = [tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.F1Score(), tf.keras.metrics.CategoricalCrossentropy()])\n",
        "\n",
        "\n",
        "# model = tf.keras.saving.load_model(\"/ckpt/checkpoint.model.keras\")            # load the model from checkpoint to be trained\n",
        "\n",
        "\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            optimizer= tf.keras.optimizers.AdamW(learning_rate = 5.6761842602901e-05, global_clipnorm=1, amsgrad = True, weight_decay=5.468661421085422e-05),\n",
        "            metrics = [tf.keras.metrics.CategoricalCrossentropy(),tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.F1Score()])\n",
        "\n",
        "# save model checkpoints\n",
        "checkpoint_filepath = 'ckpt/epoch:{epoch:02d}-val_loss:{val_loss:.4f}.keras'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(                  # model checkpoint callback\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose = 0)\n",
        "\n",
        "# earlystopping callback\n",
        "EStop = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_crossentropy\", mode = 'min', min_delta= 0.0001, patience=500, restore_best_weights=True, verbose=0)\n",
        "\n",
        "# assigning weights for the classes (untuned)\n",
        "weight_for_0 = 1\n",
        "weight_for_1 = 1\n",
        "weight_for_2 = 1\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
        "\n",
        "# # Fitting the RNN to the Training set\n",
        "model.fit(x=X_train, y=Y_train, validation_data = (X_val,y_val) ,epochs = 4000, batch_size=150, verbose=2, class_weight=class_weight, callbacks=[model_checkpoint_callback, EStop])\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate\n",
        "model = tf.keras.models.load_model('/home/samer/Desktop/Big data Small Data/BDSD/Minor_project/BDSD_Minor_Project/trained_models/epoch4437val_loss0.6506.keras')            # load the model to be evaluated\n",
        "tf.keras.utils.plot_model(model,                      # plot the model\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True,\n",
        "    show_layer_activations=True,\n",
        "    show_trainable=True, show_shapes=True, rankdir=\"LR\", to_file='foto.png')\n",
        "\n",
        "test_blend_set = []\n",
        "test_labels_set = []\n",
        "test_index_set = []\n",
        "with open(\"blends_test_index.csv\", mode= \"r\") as test_data:             # load the test set\n",
        "  csvFile = csv.reader(test_data)\n",
        "  next(csvFile)\n",
        "  for lines in csvFile:\n",
        "      test_blend_set.append(lines[0:52])\n",
        "      test_labels_set.append(lines[52])\n",
        "      test_index_set.append(lines[53])\n",
        "test_blend_set = np.array(test_blend_set, dtype=np.float64)\n",
        "test_labels_set = np.array(test_labels_set, dtype=np.float64).astype('int')\n",
        "test_labels_set = tf.keras.utils.to_categorical(test_labels_set, num_classes=3)\n",
        "X_test = np.reshape(test_blend_set, (1646, 52,1))\n",
        "\n",
        "model.evaluate(X_test,test_labels_set)                          # evaluate the model using the test set\n",
        "\n",
        "\n",
        "model.save('LSTM_model_full_data_acc:63_f1:55.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict test classes to beused in further analysis\n",
        "\n",
        "# new_model = tf.keras.models.load_model('LSTM_model_73%_test_acc')\n",
        "predictions = []\n",
        "for inst in X_test:\n",
        "    inst = np.array(inst, dtype=np.float64)\n",
        "    inst = np.reshape(inst, (1,52,1))\n",
        "\n",
        "    y_pred = model.predict(inst, verbose=0)\n",
        "    y_pred = np.argmax(y_pred)\n",
        "    predictions.append(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# confusion matrix and error counting\n",
        "\n",
        "errors_count = {\"0\":0, \"1\":0, \"2\":0}\n",
        "ground_truth = []\n",
        "errors = []\n",
        "for i in test_labels_set:\n",
        "    i = np.argmax(i)\n",
        "    ground_truth.append(i)\n",
        "for j in range(len(test_blend_set)):\n",
        "    if predictions[j] != ground_truth[j]:\n",
        "        errors.append(test_index_set[j])             # creat a list of the indices of the misclassified images in the testset\n",
        "cm=tf.math.confusion_matrix(ground_truth,predictions,num_classes=3,dtype=tf.dtypes.int32,)        # calculate the confusion matrix\n",
        "\n",
        "for i in range(1646):\n",
        "    if predictions[i]==ground_truth[i]:\n",
        "        continue\n",
        "    elif predictions[i] != ground_truth[i]:\n",
        "        errors_count[str(ground_truth[i])] = errors_count[str(ground_truth[i])] + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize misclassified images from the test set\n",
        "\n",
        "test_blend_set = []\n",
        "with open(\"test_set_full_index.csv\", mode= \"r\") as test_data:\n",
        "  csvFile = csv.reader(test_data)\n",
        "  next(csvFile)\n",
        "  for lines in csvFile:\n",
        "      test_blend_set.append(lines[0:52])\n",
        "      # imageee = np.array(str(lines[1]).split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "      # imageee = cv2.cvtColor(imageee, cv2.COLOR_GRAY2RGB)\n",
        "      # plt.imshow(imageee)\n",
        "      # plt.show()\n",
        "for ind in errors:\n",
        "      print(ind)\n",
        "      imageee = np.array(str(test_blend_set[int(float(ind))][1]).split(' ')).reshape(48, 48, 1).astype(np.uint8)\n",
        "      imageees = cv2.cvtColor(imageee, cv2.COLOR_GRAY2RGB)\n",
        "      frame = mp.Image(image_format=mp.ImageFormat.SRGB,data=imageees)\n",
        "      detection_result = detector.detect(frame)\n",
        "      annotated_image = draw_landmarks_on_image(frame.numpy_view(), detection_result)\n",
        "      plt.imshow(annotated_image)\n",
        "      plt.show()\n",
        "      plt.imshow(imageees)\n",
        "      plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNUX/JCDJTXmt8IxZd2p+PS",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
